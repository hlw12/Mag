{
  "model_architecture": "AdvancedMagnitudeNet(\n  (time_branch): EnhancedTimeBranch(\n    (multi_scale1): MultiScaleConvBlock(\n      (conv1): Conv1d(3, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(3, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n      (conv3): Conv1d(3, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n      (conv4): Conv1d(3, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (residual): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n    )\n    (multi_scale2): MultiScaleConvBlock(\n      (conv1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n      (conv3): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n      (conv4): Conv1d(64, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (residual): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n    )\n    (multi_scale3): MultiScaleConvBlock(\n      (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(128, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n      (conv3): Conv1d(128, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n      (conv4): Conv1d(128, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (residual): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n    )\n    (adaptive_pool): AdaptiveAvgPool1d(output_size=128)\n    (transformer): TransformerEncoder(\n      (layers): ModuleList(\n        (0-2): 3 x TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n          )\n          (linear1): Linear(in_features=256, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=512, out_features=256, bias=True)\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (magnitude_pool): MagnitudeAwarePooling(\n      (energy_fc): Linear(in_features=1, out_features=3, bias=True)\n    )\n  )\n  (freq_branch): EnhancedFreqBranch(\n    (conv1): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): GELU(approximate='none')\n      (3): AdaptiveSpectralAttention(\n        (freq_pool): AdaptiveAvgPool2d(output_size=(1, None))\n        (time_pool): AdaptiveAvgPool2d(output_size=(None, 1))\n        (freq_attention): Sequential(\n          (0): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n          (1): ReLU()\n          (2): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1))\n          (3): Sigmoid()\n        )\n        (time_attention): Sequential(\n          (0): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n          (1): ReLU()\n          (2): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1))\n          (3): Sigmoid()\n        )\n      )\n    )\n    (conv2): Sequential(\n      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): GELU(approximate='none')\n      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (4): AdaptiveSpectralAttention(\n        (freq_pool): AdaptiveAvgPool2d(output_size=(1, None))\n        (time_pool): AdaptiveAvgPool2d(output_size=(None, 1))\n        (freq_attention): Sequential(\n          (0): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n          (1): ReLU()\n          (2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n          (3): Sigmoid()\n        )\n        (time_attention): Sequential(\n          (0): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n          (1): ReLU()\n          (2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n          (3): Sigmoid()\n        )\n      )\n    )\n    (conv3): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): GELU(approximate='none')\n      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (4): AdaptiveSpectralAttention(\n        (freq_pool): AdaptiveAvgPool2d(output_size=(1, None))\n        (time_pool): AdaptiveAvgPool2d(output_size=(None, 1))\n        (freq_attention): Sequential(\n          (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n          (1): ReLU()\n          (2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))\n          (3): Sigmoid()\n        )\n        (time_attention): Sequential(\n          (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n          (1): ReLU()\n          (2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))\n          (3): Sigmoid()\n        )\n      )\n    )\n    (conv4): Sequential(\n      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): GELU(approximate='none')\n      (3): AdaptiveAvgPool2d(output_size=(4, 4))\n    )\n    (dropout): Dropout2d(p=0.1, inplace=False)\n  )\n  (cross_fusion): CrossModalFusion(\n    (time_proj): Linear(in_features=256, out_features=512, bias=True)\n    (freq_proj): Linear(in_features=4096, out_features=512, bias=True)\n    (cross_attention): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n    )\n    (fusion_net): Sequential(\n      (0): Linear(in_features=1024, out_features=512, bias=True)\n      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (2): GELU(approximate='none')\n      (3): Dropout(p=0.1, inplace=False)\n      (4): Linear(in_features=512, out_features=256, bias=True)\n    )\n  )\n  (regression_head): MagnitudeRegressionHead(\n    (layers): Sequential(\n      (0): Linear(in_features=256, out_features=256, bias=True)\n      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (2): GELU(approximate='none')\n      (3): Dropout(p=0.1, inplace=False)\n      (4): Linear(in_features=256, out_features=128, bias=True)\n      (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (6): GELU(approximate='none')\n      (7): Dropout(p=0.1, inplace=False)\n      (8): Linear(in_features=128, out_features=64, bias=True)\n      (9): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      (10): GELU(approximate='none')\n      (11): Dropout(p=0.0, inplace=False)\n      (12): Linear(in_features=64, out_features=1, bias=True)\n    )\n  )\n)",
  "training_parameters": {
    "learning_rate": "N/A",
    "batch_size": "N/A",
    "epochs": 231
  }
}